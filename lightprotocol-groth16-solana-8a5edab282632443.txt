Directory structure:
└── lightprotocol-groth16-solana/
    ├── README.md
    ├── Cargo.toml
    ├── LICENSE
    ├── package.json
    ├── parse_vk_to_rust.js
    ├── .prettierrc
    ├── src/
    │   ├── decompression.rs
    │   ├── errors.rs
    │   ├── groth16.rs
    │   ├── lib.rs
    │   ├── proof_parser.rs
    │   └── vk_parser.rs
    ├── tests/
    │   └── rust-vk/
    │       ├── README.md
    │       ├── build.rs
    │       ├── Cargo.toml
    │       ├── package.json
    │       ├── circuits/
    │       │   ├── README.md
    │       │   └── compressed_account_merkle_proof.circom
    │       ├── scripts/
    │       │   └── setup.sh
    │       └── src/
    │           ├── lib.rs
    │           └── verifying_key.rs
    └── .github/
        ├── actions/
        │   └── setup/
        │       └── action.yml
        └── workflows/
            └── rust-tests.yml

================================================
FILE: README.md
================================================
# groth16-solana

Groth16 zero-knowledge proof verification with Solana altbn254 syscalls.

Verification takes less than 200,000 compute units.

The syscalls are contained in Solana releases 1.18.x onwards and are active on Solana Mainnet-beta.

Inputs need to be in u8 arrays in big endian.

See functional test as an example how to use this library.

This crate is compatible with Groth16 proofs of circom circuits.

The verifier file can be generated with the JavaScript script from a verifyingkey.json file generated by snarkjs.

## Usage:

```rust
let mut public_inputs_vec = Vec::new();
for input in PUBLIC_INPUTS.chunks(32) {
    public_inputs_vec.push(input);
}
let proof_a: G1 =
    <G1 as FromBytes>::read(&*[&change_endianness(&PROOF[0..64])[..], &[0u8][..]].concat())
        .unwrap();
let mut proof_a_neg = [0u8; 65];
<G1 as ToBytes>::write(&proof_a.neg(), &mut proof_a_neg[..]).unwrap();
let proof_a = change_endianness(&proof_a_neg[..64]).try_into().unwrap();
let proof_b = PROOF[64..192].try_into().unwrap();
let proof_c = PROOF[192..256].try_into().unwrap();
let mut verifier = Groth16Verifier::new(
    &proof_a,
    &proof_b,
    &proof_c,
    public_inputs_vec.as_slice(),
    &VERIFYING_KEY,
)
.unwrap();
verifier.verify().unwrap();
```

See functional test for a running example how to use this library.

## Create Verifyingkey from snarkjs verifyingKey.json

Use snarkjs to export the verifyingkey as json.

In this repo:

- npm i
- npm run parse-vk <inputFile>

## Audit
The groth16_solana release 0.0.1 has been audited during the Light Protocol v3 audit. Check out the report [here](https://file.notion.so/f/f/3e18f32c-2f42-4786-8870-c571eb0af77e/ebf1b371-2456-4127-b419-1a9812108368/Light_Protocol_V3_Audit_Report.pdf?id=2169256e-e998-4d50-a922-4602a20fe65b&table=block&spaceId=3e18f32c-2f42-4786-8870-c571eb0af77e&expirationTimestamp=1722110400000&signature=Q4NG6VMKx8UqG-xze7eKwdYGINTlIoC7-TI49wGJGSU&downloadName=Light+Protocol+V3+Audit+Report.pdf). 

Note: This open-source crate is provided "as-is" without warranties. Use at your own risk.

## License

Licensed under [Apache License, Version 2.0](LICENSE).

### Contribution

Unless you explicitly state otherwise, any contribution intentionally
submitted for inclusion in the work by you, as defined in the Apache-2.0
license, shall be dual licensed as above, without any additional terms or
conditions.



================================================
FILE: Cargo.toml
================================================
[workspace]
members = [".", "tests/rust-vk"]
resolver = "2"

[package]
name = "groth16-solana"
version = "0.2.0"
edition = "2021"
description = "groth16 verification with solana alt_bn128 syscalls"
repository = "https://github.com/Lightprotocol/groth16-solana"
license = "MIT"

[dependencies]
solana-bn254 = "2"
thiserror = "1.0"
ark-serialize = "0.5"
ark-ec = "0.5"
ark-ff = "0.5"
ark-bn254 = "0.5"
num-bigint = "0.4.6"
serde = { version = "1.0.195", optional = true, features = ["derive"] }
serde_json = { version = "1.0.111", optional = true }
circom-prover = { version = "0.1", optional = true }
ark-groth16 = { version = "0.5", optional = true }


[dev-dependencies]
ark-std = "0.5"
array-bytes = "6.2.2"
serde = "1.0.195"
serde_json = "1.0.111"

[features]
vk = ["serde", "serde_json"]
circom = ["circom-prover", "ark-groth16"]



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: package.json
================================================
{
  "name": "groth16-solana",
  "version": "1.0.0",
  "description": "",
  "main": "parse_pvk_to_bytes_254.js",
  "scripts": {
    "parse-vk": "node parse_vk_to_rust.js",
    "format": "prettier --write \"*.{ts,js}\""
  },
  "author": "",
  "dependencies": {
    "ffjavascript": "^0.2.63"
  }
}



================================================
FILE: parse_vk_to_rust.js
================================================
var ffjavascript = require('ffjavascript');
const {unstringifyBigInts, leInt2Buff} = ffjavascript.utils;
var fs = require("fs")
const process = require('process');

async function main() {
  let inputPath = process.argv[2];
  if (!inputPath) {
    throw new Error("inputPath not specified");
  }

  let outputPath = ""
  if (process.argv[3]) {
    outputPath += process.argv[3] +"/";
  }

  console.log = () => {};

  let file = await fs.readFile(inputPath, async function(err, fd) {
   if (err) {
      return console.error(err);
   }
   console.log("File opened successfully!");
   var mydata = JSON.parse(fd.toString());
   console.log(mydata)

   for (var i in mydata) {
     if (i == 'vk_alpha_1') {

       for (var j in mydata[i]) {
         mydata[i][j] = leInt2Buff(unstringifyBigInts(mydata[i][j]), 32).reverse()
       }
     } else if (i == 'vk_beta_2') {
       for (var j in mydata[i]) {
         console.log("mydata[i][j] ", mydata[i][j])

         let tmp = Array.from(leInt2Buff(unstringifyBigInts(mydata[i][j][0]), 32)).concat(Array.from(leInt2Buff(unstringifyBigInts(mydata[i][j][1]), 32))).reverse()
         console.log("tmp ", tmp);
         mydata[i][j][0] = tmp.slice(0,32)
         mydata[i][j][1] = tmp.slice(32,64)
       }
     } else if (i == 'vk_gamma_2') {
       for (var j in mydata[i]) {
         let tmp = Array.from(leInt2Buff(unstringifyBigInts(mydata[i][j][0]), 32)).concat(Array.from(leInt2Buff(unstringifyBigInts(mydata[i][j][1]), 32))).reverse()
         console.log(`i ${i}, tmp ${tmp}`)
         mydata[i][j][0] = tmp.slice(0,32)
         mydata[i][j][1] = tmp.slice(32,64)
       }
     } else if (i == 'vk_delta_2') {
       for (var j in mydata[i]) {
         let tmp = Array.from(leInt2Buff(unstringifyBigInts(mydata[i][j][0]), 32)).concat(Array.from(leInt2Buff(unstringifyBigInts(mydata[i][j][1]), 32))).reverse()
         mydata[i][j][0] = tmp.slice(0,32)
         mydata[i][j][1] = tmp.slice(32,64)
       }
     }
     else if (i == 'vk_alphabeta_12') {
       for (var j in mydata[i]) {
         for (var z in mydata[i][j]){
           for (var u in mydata[i][j][z]){
             mydata[i][j][z][u] = leInt2Buff(unstringifyBigInts(mydata[i][j][z][u]))

           }
         }
       }
     }


     else if (i == 'IC') {
       for (var j in mydata[i]) {
         for (var z in mydata[i][j]){
            mydata[i][j][z] = leInt2Buff(unstringifyBigInts(mydata[i][j][z]), 32).reverse()

         }
       }
     }

   }


   let resFile = await fs.openSync(outputPath + "verifying_key.rs","w")
   let s = `use groth16_solana::groth16::Groth16Verifyingkey;\n\npub const VERIFYINGKEY: Groth16Verifyingkey =  Groth16Verifyingkey {\n\tnr_pubinputs: ${mydata.IC.length - 1},\n\n`
   s += "\tvk_alpha_g1: [\n"
   for (var j = 0; j < mydata.vk_alpha_1.length -1 ; j++) {
     console.log(typeof(mydata.vk_alpha_1[j]))
     s += "\t\t" + Array.from(mydata.vk_alpha_1[j])/*.reverse().toString()*/ + ",\n"
   }
   s += "\t],\n\n"
   fs.writeSync(resFile,s)
   s = "\tvk_beta_g2: [\n"
   for (var j = 0; j < mydata.vk_beta_2.length -1 ; j++) {
     for (var z = 0; z < 2; z++) {
       s += "\t\t" + Array.from(mydata.vk_beta_2[j][z])/*.reverse().toString()*/ + ",\n"
     }
   }
   s += "\t],\n\n"
   fs.writeSync(resFile,s)
   s = "\tvk_gamma_g2: [\n"
   for (var j = 0; j < mydata.vk_gamma_2.length -1 ; j++) {
     for (var z = 0; z < 2; z++) {
       s += "\t\t" + Array.from(mydata.vk_gamma_2[j][z])/*.reverse().toString()*/ + ",\n"
     }
   }
   s += "\t],\n\n"
   fs.writeSync(resFile,s)

   s = "\tvk_delta_g2: [\n"
   for (var j = 0; j < mydata.vk_delta_2.length -1 ; j++) {
     for (var z = 0; z < 2; z++) {
       s += "\t\t" + Array.from(mydata.vk_delta_2[j][z])/*.reverse().toString()*/ + ",\n"
     }
   }
   s += "\t],\n\n"
   fs.writeSync(resFile,s)
   s = "\tvk_ic: &[\n"
   let x = 0;

   for (var ic in mydata.IC) {
     s += "\t\t[\n"
     // console.log(mydata.IC[ic])
     for (var j = 0; j < mydata.IC[ic].length - 1 ; j++) {
       s += "\t\t\t" + mydata.IC[ic][j]/*.reverse().toString()*/ + ",\n"
     }
     x++;
     s += "\t\t],\n"
   }
   s += "\t]\n};"

   fs.writeSync(resFile,s)
 });
}


main()



================================================
FILE: .prettierrc
================================================
{
  "trailingComma": "all",
  "tabWidth": 2,
  "singleQuote": false
}



================================================
FILE: src/decompression.rs
================================================
use crate::errors::Groth16Error;
use solana_bn254::compression::prelude::{alt_bn128_g1_decompress, alt_bn128_g2_decompress};

pub fn decompress_g1(g1_bytes: &[u8; 32]) -> Result<[u8; 64], Groth16Error> {
    let decompressed_g1 = alt_bn128_g1_decompress(g1_bytes)
        .map_err(|_| crate::errors::Groth16Error::DecompressingG1Failed {})?;
    Ok(decompressed_g1)
}

pub fn decompress_g2(g2_bytes: &[u8; 64]) -> Result<[u8; 128], Groth16Error> {
    let decompressed_g2 = alt_bn128_g2_decompress(g2_bytes)
        .map_err(|_| crate::errors::Groth16Error::DecompressingG2Failed {})?;
    Ok(decompressed_g2)
}

#[cfg(test)]
mod tests {

    use super::*;
    use ark_bn254;
    use ark_serialize::{CanonicalDeserialize, CanonicalSerialize, Compress, Validate};
    use solana_bn254::compression::prelude::convert_endianness;

    use ark_serialize::Flags;
    type G1 = ark_bn254::g1::G1Affine;
    type G2 = ark_bn254::g2::G2Affine;

    pub const PROOF: [u8; 256] = [
        45, 206, 255, 166, 152, 55, 128, 138, 79, 217, 145, 164, 25, 74, 120, 234, 234, 217, 68,
        149, 162, 44, 133, 120, 184, 205, 12, 44, 175, 98, 168, 172, 20, 24, 216, 15, 209, 175,
        106, 75, 147, 236, 90, 101, 123, 219, 245, 151, 209, 202, 218, 104, 148, 8, 32, 254, 243,
        191, 218, 122, 42, 81, 193, 84, 40, 57, 233, 205, 180, 46, 35, 111, 215, 5, 23, 93, 12, 71,
        118, 225, 7, 46, 247, 147, 47, 130, 106, 189, 184, 80, 146, 103, 141, 52, 242, 25, 0, 203,
        124, 176, 110, 34, 151, 212, 66, 180, 238, 151, 236, 189, 133, 209, 17, 137, 205, 183, 168,
        196, 92, 159, 75, 174, 81, 168, 18, 86, 176, 56, 16, 26, 210, 20, 18, 81, 122, 142, 104,
        62, 251, 169, 98, 141, 21, 253, 50, 130, 182, 15, 33, 109, 228, 31, 79, 183, 88, 147, 174,
        108, 4, 22, 14, 129, 168, 6, 80, 246, 254, 100, 218, 131, 94, 49, 247, 211, 3, 245, 22,
        200, 177, 91, 60, 144, 147, 174, 90, 17, 19, 189, 62, 147, 152, 18, 41, 139, 183, 208, 246,
        198, 118, 127, 89, 160, 9, 27, 61, 26, 123, 180, 221, 108, 17, 166, 47, 115, 82, 48, 132,
        139, 253, 65, 152, 92, 209, 53, 37, 25, 83, 61, 252, 42, 181, 243, 16, 21, 2, 199, 123, 96,
        218, 151, 253, 86, 69, 181, 202, 109, 64, 129, 124, 254, 192, 25, 177, 199, 26, 50,
    ];
    #[test]
    fn apply_bitmask() {
        let proof_a_le: G1 = G1::deserialize_with_mode(
            &convert_endianness::<32, 64>(&PROOF[0..64].try_into().unwrap())[..],
            Compress::No,
            Validate::Yes,
        )
        .unwrap();

        let index = 31;
        let mask = proof_a_le.to_flags().u8_bitmask();
        let mut new_proof_a_bytes =
            convert_endianness::<32, 64>(&PROOF[0..64].try_into().unwrap()).to_vec();
        new_proof_a_bytes[index] |= mask;
        let proof_a_compressed_be: [u8; 32] =
            convert_endianness::<32, 64>(&new_proof_a_bytes.try_into().unwrap())[0..32]
                .try_into()
                .unwrap();
        let proof_a_be = decompress_g1(&proof_a_compressed_be).unwrap();
        assert_eq!(proof_a_be.to_vec(), PROOF[0..64].to_vec());

        let index = 63;
        let le_proof_b_bytes = convert_endianness::<64, 128>(&PROOF[64..192].try_into().unwrap());
        let mut new_proof_b_bytes = le_proof_b_bytes[0..64].to_vec();
        let proof_b_uncompressed =
            G2::deserialize_with_mode(&le_proof_b_bytes[..], Compress::No, Validate::Yes).unwrap();
        let mask = proof_b_uncompressed.to_flags().u8_bitmask();

        new_proof_b_bytes[index] |= mask;

        let mut serialized_compressed = [0u8; 64];
        G2::serialize_compressed(&proof_b_uncompressed, serialized_compressed.as_mut()).unwrap();

        assert_eq!(
            serialized_compressed[0..32].to_vec(),
            le_proof_b_bytes[..32]
        );
        assert_eq!(
            serialized_compressed[32..64].to_vec(),
            le_proof_b_bytes[32..64]
        );
        assert_eq!(
            serialized_compressed[0..32].to_vec(),
            new_proof_b_bytes[..32]
        );
        assert_eq!(
            serialized_compressed[32..64].to_vec(),
            new_proof_b_bytes[32..64]
        );

        let proof_b = decompress_g2(&convert_endianness::<64, 64>(
            &new_proof_b_bytes.try_into().unwrap(),
        ))
        .unwrap();
        assert_eq!(proof_b.to_vec(), PROOF[64..192].to_vec());

        let index = 31;
        let proof_c_uncompressed = G1::deserialize_uncompressed(
            &convert_endianness::<32, 64>(&PROOF[192..].try_into().unwrap())[..],
        )
        .unwrap();

        let mask = proof_c_uncompressed.to_flags().u8_bitmask();
        let mut new_proof_c_bytes =
            convert_endianness::<32, 64>(&PROOF[192..].try_into().unwrap()).to_vec();
        new_proof_c_bytes[index] |= mask;
        let mut serialized_compressed = [0u8; 32];
        G1::serialize_compressed(&proof_c_uncompressed, serialized_compressed.as_mut()).unwrap();
        assert_eq!(serialized_compressed.to_vec(), new_proof_c_bytes[..32]);
        let proof_c = decompress_g1(&convert_endianness::<32, 32>(
            &new_proof_c_bytes[0..32].try_into().unwrap(),
        ))
        .unwrap();
        assert_eq!(proof_c.to_vec(), PROOF[192..].to_vec());
    }
}



================================================
FILE: src/errors.rs
================================================
use thiserror::Error;

#[derive(Debug, Error, PartialEq)]
pub enum Groth16Error {
    #[error("Incompatible Verifying Key with number of public inputs")]
    IncompatibleVerifyingKeyWithNrPublicInputs,
    #[error("ProofVerificationFailed")]
    ProofVerificationFailed,
    #[error("PreparingInputsG1AdditionFailed")]
    PreparingInputsG1AdditionFailed,
    #[error("PreparingInputsG1MulFailed")]
    PreparingInputsG1MulFailed,
    #[error("InvalidG1Length")]
    InvalidG1Length,
    #[error("InvalidG2Length")]
    InvalidG2Length,
    #[error("InvalidPublicInputsLength")]
    InvalidPublicInputsLength,
    #[error("DecompressingG1Failed")]
    DecompressingG1Failed,
    #[error("DecompressingG2Failed")]
    DecompressingG2Failed,
    #[error("PublicInputGreaterThanFieldSize")]
    PublicInputGreaterThanFieldSize,
    #[cfg(feature = "circom")]
    #[error("Arkworks serialization error: {0}")]
    ArkworksSerializationError(String),
    #[cfg(feature = "circom")]
    #[error("Failed to convert proof component to byte array")]
    ProofConversionError,
}

#[cfg(feature = "circom")]
impl From<ark_serialize::SerializationError> for Groth16Error {
    fn from(e: ark_serialize::SerializationError) -> Self {
        Groth16Error::ArkworksSerializationError(e.to_string())
    }
}

impl From<Groth16Error> for u32 {
    fn from(error: Groth16Error) -> Self {
        match error {
            Groth16Error::IncompatibleVerifyingKeyWithNrPublicInputs => 0,
            Groth16Error::ProofVerificationFailed => 1,
            Groth16Error::PreparingInputsG1AdditionFailed => 2,
            Groth16Error::PreparingInputsG1MulFailed => 3,
            Groth16Error::InvalidG1Length => 4,
            Groth16Error::InvalidG2Length => 5,
            Groth16Error::InvalidPublicInputsLength => 6,
            Groth16Error::DecompressingG1Failed => 7,
            Groth16Error::DecompressingG2Failed => 8,
            Groth16Error::PublicInputGreaterThanFieldSize => 9,
            #[cfg(feature = "circom")]
            Groth16Error::ArkworksSerializationError(_) => 10,
            #[cfg(feature = "circom")]
            Groth16Error::ProofConversionError => 11,
        }
    }
}



================================================
FILE: src/groth16.rs
================================================
//! ```rust,ignore
//! let mut public_inputs_vec = Vec::new();
//! for input in PUBLIC_INPUTS.chunks(32) {
//!     public_inputs_vec.push(input);
//! }
//!
//! let proof_a: G1 =
//!     <G1 as FromBytes>::read(&*[&change_endianness(&PROOF[0..64])[..], &[0u8][..]].concat())?;
//! let mut proof_a_neg = [0u8; 65];
//! <G1 as ToBytes>::write(&proof_a.neg(), &mut proof_a_neg[..])?;
//!
//! let proof_a = change_endianness(&proof_a_neg[..64]).try_into()?;
//! let proof_b = PROOF[64..192].try_into()?;
//! let proof_c = PROOF[192..256].try_into()?;
//!
//! let mut verifier = Groth16Verifier::new(
//!     &proof_a,
//!     &proof_b,
//!     &proof_c,
//!     public_inputs_vec.as_slice(),
//!     &VERIFYING_KEY,
//! )?;
//! verifier.verify()?;
//! ```
//!
//! See functional test for a running example how to use this library.
//!
use crate::errors::Groth16Error;
use ark_ff::PrimeField;
use num_bigint::BigUint;
use solana_bn254::prelude::{alt_bn128_addition, alt_bn128_multiplication, alt_bn128_pairing};

#[derive(PartialEq, Eq, Debug)]
pub struct Groth16Verifyingkey<'a> {
    pub nr_pubinputs: usize,
    pub vk_alpha_g1: [u8; 64],
    pub vk_beta_g2: [u8; 128],
    pub vk_gamma_g2: [u8; 128],
    pub vk_delta_g2: [u8; 128],
    pub vk_ic: &'a [[u8; 64]],
}

#[derive(PartialEq, Eq, Debug)]
pub struct Groth16Verifier<'a, const NR_INPUTS: usize> {
    proof_a: &'a [u8; 64],
    proof_b: &'a [u8; 128],
    proof_c: &'a [u8; 64],
    public_inputs: &'a [[u8; 32]; NR_INPUTS],
    prepared_public_inputs: [u8; 64],
    verifyingkey: &'a Groth16Verifyingkey<'a>,
}

impl<const NR_INPUTS: usize> Groth16Verifier<'_, NR_INPUTS> {
    pub fn new<'a>(
        proof_a: &'a [u8; 64],
        proof_b: &'a [u8; 128],
        proof_c: &'a [u8; 64],
        public_inputs: &'a [[u8; 32]; NR_INPUTS],
        verifyingkey: &'a Groth16Verifyingkey<'a>,
    ) -> Result<Groth16Verifier<'a, NR_INPUTS>, Groth16Error> {
        if proof_a.len() != 64 {
            return Err(Groth16Error::InvalidG1Length);
        }

        if proof_b.len() != 128 {
            return Err(Groth16Error::InvalidG2Length);
        }

        if proof_c.len() != 64 {
            return Err(Groth16Error::InvalidG1Length);
        }

        if public_inputs.len() + 1 != verifyingkey.vk_ic.len() {
            return Err(Groth16Error::InvalidPublicInputsLength);
        }

        Ok(Groth16Verifier {
            proof_a,
            proof_b,
            proof_c,
            public_inputs,
            prepared_public_inputs: [0u8; 64],
            verifyingkey,
        })
    }

    pub fn prepare_inputs<const CHECK: bool>(&mut self) -> Result<(), Groth16Error> {
        let mut prepared_public_inputs = self.verifyingkey.vk_ic[0];

        for (i, input) in self.public_inputs.iter().enumerate() {
            if CHECK && !is_less_than_bn254_field_size_be(input) {
                return Err(Groth16Error::PublicInputGreaterThanFieldSize);
            }
            let mul_res = alt_bn128_multiplication(
                &[&self.verifyingkey.vk_ic[i + 1][..], &input[..]].concat(),
            )
            .map_err(|_| Groth16Error::PreparingInputsG1MulFailed)?;
            prepared_public_inputs =
                alt_bn128_addition(&[&mul_res[..], &prepared_public_inputs[..]].concat())
                    .map_err(|_| Groth16Error::PreparingInputsG1AdditionFailed)?[..]
                    .try_into()
                    .map_err(|_| Groth16Error::PreparingInputsG1AdditionFailed)?;
        }

        self.prepared_public_inputs = prepared_public_inputs;

        Ok(())
    }

    /// Verifies the proof, and checks that public inputs are smaller than
    /// field size.
    pub fn verify(&mut self) -> Result<(), Groth16Error> {
        self.verify_common::<true>()
    }

    /// Verifies the proof, and does not check that public inputs are smaller
    /// than field size.
    pub fn verify_unchecked(&mut self) -> Result<(), Groth16Error> {
        self.verify_common::<false>()
    }

    fn verify_common<const CHECK: bool>(&mut self) -> Result<(), Groth16Error> {
        self.prepare_inputs::<CHECK>()?;

        let pairing_input = [
            self.proof_a.as_slice(),
            self.proof_b.as_slice(),
            self.prepared_public_inputs.as_slice(),
            self.verifyingkey.vk_gamma_g2.as_slice(),
            self.proof_c.as_slice(),
            self.verifyingkey.vk_delta_g2.as_slice(),
            self.verifyingkey.vk_alpha_g1.as_slice(),
            self.verifyingkey.vk_beta_g2.as_slice(),
        ]
        .concat();

        let pairing_res = alt_bn128_pairing(pairing_input.as_slice())
            .map_err(|_| Groth16Error::ProofVerificationFailed)?;

        if pairing_res[31] != 1 {
            return Err(Groth16Error::ProofVerificationFailed);
        }
        Ok(())
    }
}

pub fn is_less_than_bn254_field_size_be(bytes: &[u8; 32]) -> bool {
    let bigint = BigUint::from_bytes_be(bytes);
    bigint < ark_bn254::Fr::MODULUS.into()
}

#[cfg(test)]
mod tests {
    use crate::decompression::{decompress_g1, decompress_g2};

    use super::*;
    use ark_bn254;
    use ark_ff::BigInteger;
    use ark_serialize::{CanonicalDeserialize, CanonicalSerialize, Compress, Validate};
    use std::ops::Neg;
    type G1 = ark_bn254::g1::G1Affine;
    type G2 = ark_bn254::g2::G2Affine;
    use solana_bn254::compression::prelude::convert_endianness;

    pub const VERIFYING_KEY: Groth16Verifyingkey = Groth16Verifyingkey {
        nr_pubinputs: 10,

        vk_alpha_g1: [
            45, 77, 154, 167, 227, 2, 217, 223, 65, 116, 157, 85, 7, 148, 157, 5, 219, 234, 51,
            251, 177, 108, 100, 59, 34, 245, 153, 162, 190, 109, 242, 226, 20, 190, 221, 80, 60,
            55, 206, 176, 97, 216, 236, 96, 32, 159, 227, 69, 206, 137, 131, 10, 25, 35, 3, 1, 240,
            118, 202, 255, 0, 77, 25, 38,
        ],

        vk_beta_g2: [
            9, 103, 3, 47, 203, 247, 118, 209, 175, 201, 133, 248, 136, 119, 241, 130, 211, 132,
            128, 166, 83, 242, 222, 202, 169, 121, 76, 188, 59, 243, 6, 12, 14, 24, 120, 71, 173,
            76, 121, 131, 116, 208, 214, 115, 43, 245, 1, 132, 125, 214, 139, 192, 224, 113, 36,
            30, 2, 19, 188, 127, 193, 61, 183, 171, 48, 76, 251, 209, 224, 138, 112, 74, 153, 245,
            232, 71, 217, 63, 140, 60, 170, 253, 222, 196, 107, 122, 13, 55, 157, 166, 154, 77, 17,
            35, 70, 167, 23, 57, 193, 177, 164, 87, 168, 199, 49, 49, 35, 210, 77, 47, 145, 146,
            248, 150, 183, 198, 62, 234, 5, 169, 213, 127, 6, 84, 122, 208, 206, 200,
        ],

        vk_gamma_g2: [
            25, 142, 147, 147, 146, 13, 72, 58, 114, 96, 191, 183, 49, 251, 93, 37, 241, 170, 73,
            51, 53, 169, 231, 18, 151, 228, 133, 183, 174, 243, 18, 194, 24, 0, 222, 239, 18, 31,
            30, 118, 66, 106, 0, 102, 94, 92, 68, 121, 103, 67, 34, 212, 247, 94, 218, 221, 70,
            222, 189, 92, 217, 146, 246, 237, 9, 6, 137, 208, 88, 95, 240, 117, 236, 158, 153, 173,
            105, 12, 51, 149, 188, 75, 49, 51, 112, 179, 142, 243, 85, 172, 218, 220, 209, 34, 151,
            91, 18, 200, 94, 165, 219, 140, 109, 235, 74, 171, 113, 128, 141, 203, 64, 143, 227,
            209, 231, 105, 12, 67, 211, 123, 76, 230, 204, 1, 102, 250, 125, 170,
        ],

        vk_delta_g2: [
            25, 142, 147, 147, 146, 13, 72, 58, 114, 96, 191, 183, 49, 251, 93, 37, 241, 170, 73,
            51, 53, 169, 231, 18, 151, 228, 133, 183, 174, 243, 18, 194, 24, 0, 222, 239, 18, 31,
            30, 118, 66, 106, 0, 102, 94, 92, 68, 121, 103, 67, 34, 212, 247, 94, 218, 221, 70,
            222, 189, 92, 217, 146, 246, 237, 9, 6, 137, 208, 88, 95, 240, 117, 236, 158, 153, 173,
            105, 12, 51, 149, 188, 75, 49, 51, 112, 179, 142, 243, 85, 172, 218, 220, 209, 34, 151,
            91, 18, 200, 94, 165, 219, 140, 109, 235, 74, 171, 113, 128, 141, 203, 64, 143, 227,
            209, 231, 105, 12, 67, 211, 123, 76, 230, 204, 1, 102, 250, 125, 170,
        ],

        vk_ic: &[
            [
                3, 183, 175, 189, 219, 73, 183, 28, 132, 200, 83, 8, 65, 22, 184, 81, 82, 36, 181,
                186, 25, 216, 234, 25, 151, 2, 235, 194, 13, 223, 32, 145, 15, 37, 113, 122, 93,
                59, 91, 25, 236, 104, 227, 238, 58, 154, 67, 250, 186, 91, 93, 141, 18, 241, 150,
                59, 202, 48, 179, 1, 53, 207, 155, 199,
            ],
            [
                46, 253, 85, 84, 166, 240, 71, 175, 111, 174, 244, 62, 87, 96, 235, 196, 208, 85,
                186, 47, 163, 237, 53, 204, 176, 190, 62, 201, 189, 216, 132, 71, 6, 91, 228, 97,
                74, 5, 0, 255, 147, 113, 161, 152, 238, 177, 78, 81, 111, 13, 142, 220, 24, 133,
                27, 149, 66, 115, 34, 87, 224, 237, 44, 162,
            ],
            [
                29, 157, 232, 254, 238, 178, 82, 15, 152, 205, 175, 129, 90, 108, 114, 60, 82, 162,
                37, 234, 115, 69, 191, 125, 212, 85, 176, 176, 113, 41, 23, 84, 8, 229, 196, 41,
                191, 243, 112, 105, 166, 75, 113, 160, 140, 34, 139, 179, 53, 180, 245, 195, 5, 24,
                42, 18, 82, 60, 173, 192, 67, 149, 211, 250,
            ],
            [
                18, 4, 92, 105, 55, 33, 222, 133, 144, 185, 99, 131, 167, 143, 52, 120, 44, 79,
                164, 63, 119, 223, 199, 154, 26, 86, 22, 208, 50, 53, 159, 65, 14, 171, 53, 159,
                255, 133, 91, 30, 162, 209, 152, 18, 251, 112, 105, 90, 65, 234, 44, 4, 42, 173,
                31, 230, 229, 137, 177, 112, 241, 142, 62, 176,
            ],
            [
                13, 117, 56, 250, 131, 38, 119, 205, 221, 228, 32, 185, 236, 82, 102, 29, 198, 53,
                117, 151, 19, 10, 255, 211, 41, 210, 72, 221, 79, 107, 251, 150, 35, 187, 30, 32,
                198, 17, 220, 4, 68, 10, 71, 51, 31, 169, 4, 174, 10, 38, 227, 229, 193, 129, 150,
                76, 94, 224, 182, 13, 166, 65, 175, 89,
            ],
            [
                21, 167, 160, 214, 213, 132, 208, 197, 115, 195, 129, 111, 129, 38, 56, 52, 41, 57,
                72, 249, 50, 187, 184, 49, 240, 228, 142, 147, 187, 96, 96, 102, 34, 163, 43, 218,
                199, 187, 250, 245, 119, 151, 237, 67, 231, 70, 236, 67, 157, 181, 216, 174, 25,
                82, 120, 255, 191, 89, 230, 165, 179, 241, 188, 218,
            ],
            [
                4, 136, 219, 130, 55, 89, 21, 224, 41, 30, 53, 234, 66, 160, 129, 174, 154, 139,
                151, 33, 163, 221, 150, 192, 171, 102, 241, 161, 48, 130, 31, 175, 6, 47, 176, 127,
                13, 8, 36, 228, 239, 219, 6, 158, 22, 31, 22, 162, 91, 196, 132, 188, 156, 228, 30,
                1, 178, 246, 197, 186, 236, 249, 236, 147,
            ],
            [
                9, 41, 120, 80, 67, 24, 240, 221, 136, 156, 137, 182, 168, 17, 176, 118, 119, 72,
                170, 188, 227, 31, 15, 22, 252, 37, 198, 154, 195, 163, 64, 125, 37, 211, 235, 67,
                249, 133, 45, 90, 162, 9, 173, 19, 80, 154, 208, 173, 221, 203, 206, 254, 81, 197,
                104, 26, 177, 78, 86, 210, 51, 116, 60, 87,
            ],
            [
                3, 41, 86, 208, 125, 147, 53, 187, 213, 220, 195, 141, 216, 40, 92, 137, 70, 210,
                168, 103, 105, 236, 85, 37, 165, 209, 246, 75, 122, 251, 75, 93, 28, 108, 154, 181,
                15, 16, 35, 88, 65, 211, 8, 11, 123, 84, 185, 187, 184, 1, 83, 141, 67, 46, 241,
                222, 232, 135, 59, 44, 152, 217, 237, 106,
            ],
            [
                34, 98, 189, 118, 119, 197, 102, 193, 36, 150, 200, 143, 226, 60, 0, 239, 21, 40,
                5, 156, 73, 7, 247, 14, 249, 157, 2, 241, 181, 208, 144, 0, 34, 45, 86, 133, 116,
                53, 235, 160, 107, 36, 195, 125, 122, 10, 206, 88, 85, 166, 62, 150, 65, 159, 130,
                7, 255, 224, 227, 229, 206, 138, 68, 71,
            ],
        ],
    };

    fn change_endianness(bytes: &[u8]) -> Vec<u8> {
        let mut vec = Vec::new();
        for b in bytes.chunks(32) {
            for byte in b.iter().rev() {
                vec.push(*byte);
            }
        }
        vec
    }

    pub const PUBLIC_INPUTS: [[u8; 32]; 9] = [
        [
            34, 238, 251, 182, 234, 248, 214, 189, 46, 67, 42, 25, 71, 58, 145, 58, 61, 28, 116,
            110, 60, 17, 82, 149, 178, 187, 160, 211, 37, 226, 174, 231,
        ],
        [
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 51,
            152, 17, 147,
        ],
        [
            4, 247, 199, 87, 230, 85, 103, 90, 28, 183, 95, 100, 200, 46, 3, 158, 247, 196, 173,
            146, 207, 167, 108, 33, 199, 18, 13, 204, 198, 101, 223, 186,
        ],
        [
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7,
            49, 65, 41,
        ],
        [
            7, 130, 55, 65, 197, 232, 175, 217, 44, 151, 149, 225, 75, 86, 158, 105, 43, 229, 65,
            87, 51, 150, 168, 243, 176, 175, 11, 203, 180, 149, 72, 103,
        ],
        [
            46, 93, 177, 62, 42, 66, 223, 153, 51, 193, 146, 49, 154, 41, 69, 198, 224, 13, 87, 80,
            222, 171, 37, 141, 0, 1, 50, 172, 18, 28, 213, 213,
        ],
        [
            40, 141, 45, 3, 180, 200, 250, 112, 108, 94, 35, 143, 82, 63, 125, 9, 147, 37, 191, 75,
            62, 221, 138, 20, 166, 151, 219, 237, 254, 58, 230, 189,
        ],
        [
            33, 100, 143, 241, 11, 251, 73, 141, 229, 57, 129, 168, 83, 23, 235, 147, 138, 225,
            177, 250, 13, 97, 226, 162, 6, 232, 52, 95, 128, 84, 90, 202,
        ],
        [
            25, 178, 1, 208, 219, 169, 222, 123, 113, 202, 165, 77, 183, 98, 103, 237, 187, 93,
            178, 95, 169, 156, 38, 100, 125, 218, 104, 94, 104, 119, 13, 21,
        ],
    ];

    pub const PROOF: [u8; 256] = [
        45, 206, 255, 166, 152, 55, 128, 138, 79, 217, 145, 164, 25, 74, 120, 234, 234, 217, 68,
        149, 162, 44, 133, 120, 184, 205, 12, 44, 175, 98, 168, 172, 20, 24, 216, 15, 209, 175,
        106, 75, 147, 236, 90, 101, 123, 219, 245, 151, 209, 202, 218, 104, 148, 8, 32, 254, 243,
        191, 218, 122, 42, 81, 193, 84, 40, 57, 233, 205, 180, 46, 35, 111, 215, 5, 23, 93, 12, 71,
        118, 225, 7, 46, 247, 147, 47, 130, 106, 189, 184, 80, 146, 103, 141, 52, 242, 25, 0, 203,
        124, 176, 110, 34, 151, 212, 66, 180, 238, 151, 236, 189, 133, 209, 17, 137, 205, 183, 168,
        196, 92, 159, 75, 174, 81, 168, 18, 86, 176, 56, 16, 26, 210, 20, 18, 81, 122, 142, 104,
        62, 251, 169, 98, 141, 21, 253, 50, 130, 182, 15, 33, 109, 228, 31, 79, 183, 88, 147, 174,
        108, 4, 22, 14, 129, 168, 6, 80, 246, 254, 100, 218, 131, 94, 49, 247, 211, 3, 245, 22,
        200, 177, 91, 60, 144, 147, 174, 90, 17, 19, 189, 62, 147, 152, 18, 41, 139, 183, 208, 246,
        198, 118, 127, 89, 160, 9, 27, 61, 26, 123, 180, 221, 108, 17, 166, 47, 115, 82, 48, 132,
        139, 253, 65, 152, 92, 209, 53, 37, 25, 83, 61, 252, 42, 181, 243, 16, 21, 2, 199, 123, 96,
        218, 151, 253, 86, 69, 181, 202, 109, 64, 129, 124, 254, 192, 25, 177, 199, 26, 50,
    ];

    #[test]
    fn test_is_less_than_bn254_field_size_be() {
        let bytes = [0u8; 32];
        assert!(is_less_than_bn254_field_size_be(&bytes));

        let bytes: [u8; 32] = BigUint::from(ark_bn254::Fr::MODULUS)
            .to_bytes_be()
            .try_into()
            .unwrap();
        assert!(!is_less_than_bn254_field_size_be(&bytes));
    }

    #[test]
    fn proof_verification_should_succeed() {
        let proof_a: G1 = G1::deserialize_with_mode(
            &*[&change_endianness(&PROOF[0..64]), &[0u8][..]].concat(),
            Compress::No,
            Validate::Yes,
        )
        .unwrap();
        let mut proof_a_neg = [0u8; 65];
        proof_a
            .neg()
            .x
            .serialize_with_mode(&mut proof_a_neg[..32], Compress::No)
            .unwrap();
        proof_a
            .neg()
            .y
            .serialize_with_mode(&mut proof_a_neg[32..], Compress::No)
            .unwrap();

        let proof_a = change_endianness(&proof_a_neg[..64]).try_into().unwrap();
        let proof_b = PROOF[64..192].try_into().unwrap();
        let proof_c = PROOF[192..256].try_into().unwrap();

        let mut verifier =
            Groth16Verifier::new(&proof_a, &proof_b, &proof_c, &PUBLIC_INPUTS, &VERIFYING_KEY)
                .unwrap();
        verifier.verify().unwrap();
        verifier.verify_unchecked().unwrap();
    }

    fn compress_g1_be(g1: &[u8; 64]) -> [u8; 32] {
        let g1 = convert_endianness::<32, 64>(g1);
        let mut compressed = [0u8; 32];
        let g1 = G1::deserialize_with_mode(g1.as_slice(), Compress::No, Validate::Yes).unwrap();
        G1::serialize_with_mode(&g1, &mut compressed[..], Compress::Yes).unwrap();

        convert_endianness::<32, 32>(&compressed)
    }

    fn compress_g2_be(g2: &[u8; 128]) -> [u8; 64] {
        let g2: [u8; 128] = convert_endianness::<64, 128>(g2);

        let mut compressed = [0u8; 64];
        let g2 = G2::deserialize_with_mode(g2.as_slice(), Compress::No, Validate::Yes).unwrap();
        G2::serialize_with_mode(&g2, &mut compressed[..], Compress::Yes).unwrap();
        convert_endianness::<64, 64>(&compressed)
    }

    #[test]
    fn proof_verification_with_compressed_inputs_should_succeed() {
        let mut public_inputs_vec = Vec::new();
        for input in PUBLIC_INPUTS.chunks(32) {
            public_inputs_vec.push(input);
        }
        let compressed_proof_a = compress_g1_be(&PROOF[0..64].try_into().unwrap());
        let compressed_proof_b = compress_g2_be(&PROOF[64..192].try_into().unwrap());
        let compressed_proof_c = compress_g1_be(&PROOF[192..].try_into().unwrap());

        let proof_a = decompress_g1(&compressed_proof_a).unwrap();
        let proof_a: G1 = G1::deserialize_with_mode(
            &*[&change_endianness(&proof_a[0..64]), &[0u8][..]].concat(),
            Compress::No,
            Validate::Yes,
        )
        .unwrap();
        let mut proof_a_neg = [0u8; 65];
        proof_a
            .neg()
            .x
            .serialize_with_mode(&mut proof_a_neg[..32], Compress::No)
            .unwrap();
        proof_a
            .neg()
            .y
            .serialize_with_mode(&mut proof_a_neg[32..], Compress::No)
            .unwrap();

        let proof_a = change_endianness(&proof_a_neg[..64]).try_into().unwrap();
        let proof_b = decompress_g2(&compressed_proof_b).unwrap();
        let proof_c = decompress_g1(&compressed_proof_c).unwrap();
        let mut verifier =
            Groth16Verifier::new(&proof_a, &proof_b, &proof_c, &PUBLIC_INPUTS, &VERIFYING_KEY)
                .unwrap();
        verifier.verify().unwrap();
        verifier.verify_unchecked().unwrap();
    }

    #[test]
    fn wrong_proof_verification_should_not_succeed() {
        let proof_a = PROOF[0..64].try_into().unwrap();
        let proof_b = PROOF[64..192].try_into().unwrap();
        let proof_c = PROOF[192..256].try_into().unwrap();
        let mut verifier = Groth16Verifier::new(
            &proof_a, // using non negated proof a as test for wrong proof
            &proof_b,
            &proof_c,
            &PUBLIC_INPUTS,
            &VERIFYING_KEY,
        )
        .unwrap();
        assert_eq!(
            verifier.verify(),
            Err(Groth16Error::ProofVerificationFailed)
        );
        assert_eq!(
            verifier.verify_unchecked(),
            Err(Groth16Error::ProofVerificationFailed)
        );
    }

    #[test]
    fn public_input_greater_than_field_size_should_not_suceed() {
        let proof_a = PROOF[0..64].try_into().unwrap();
        let proof_b = PROOF[64..192].try_into().unwrap();
        let proof_c = PROOF[192..256].try_into().unwrap();
        let mut public_inputs = PUBLIC_INPUTS;
        public_inputs[0] = ark_bn254::Fr::MODULUS.to_bytes_be().try_into().unwrap();
        let mut verifier = Groth16Verifier::new(
            &proof_a, // using non negated proof a as test for wrong proof
            &proof_b,
            &proof_c,
            &public_inputs,
            &VERIFYING_KEY,
        )
        .unwrap();
        assert_eq!(
            verifier.verify_unchecked(),
            Err(Groth16Error::ProofVerificationFailed)
        );
        assert_eq!(
            verifier.verify(),
            Err(Groth16Error::PublicInputGreaterThanFieldSize)
        );
    }
}



================================================
FILE: src/lib.rs
================================================
//! # groth16-solana
//!
//! Groth16 zero-knowledge proof verification with Solana altbn254 syscalls.
//!
//! Verification takes less than 200,000 compute units.
//!
//! The syscalls are contained in Solana releases 1.18.x onwards and are active on mainnet-beta.
//!
//! Inputs need to be in u8 arrays in big endian.
//!
//! See functional test as an example how to use this library.
//!
//! This crate is compatible with Groth16 proofs of circom circuits.
//!
//! The verifier file can be generated with the java script script from a verifyingkey.json file generated by snarkjs.
//!
//! ## Usage:
//!
//! ```rust,ignore
//! let mut public_inputs_vec = Vec::new();
//! for input in PUBLIC_INPUTS.chunks(32) {
//!     public_inputs_vec.push(input);
//! }
//!
//! let proof_a: G1 =
//!     <G1 as FromBytes>::read(&*[&change_endianness(&PROOF[0..64])[..], &[0u8][..]].concat())
//!         .unwrap();
//! let mut proof_a_neg = [0u8; 65];
//! <G1 as ToBytes>::write(&proof_a.neg(), &mut proof_a_neg[..]).unwrap();
//!
//! let proof_a = change_endianness(&proof_a_neg[..64]).try_into().unwrap();
//! let proof_b = PROOF[64..192].try_into().unwrap();
//! let proof_c = PROOF[192..256].try_into().unwrap();
//!
//! let mut verifier = Groth16Verifier::new(
//!     &proof_a,
//!     &proof_b,
//!     &proof_c,
//!     public_inputs_vec.as_slice(),
//!     &VERIFYING_KEY,
//! )
//! .unwrap();
//! verifier.verify().unwrap();
//! ```
//!
//! See functional test for a running example how to use this library.
//!
pub mod decompression;
pub mod errors;
pub mod groth16;

#[cfg(feature = "vk")]
pub mod vk_parser;

#[cfg(feature = "circom")]
pub mod proof_parser;



================================================
FILE: src/proof_parser.rs
================================================
//! Proof parser for converting circom-prover proofs to groth16-solana format.
//!
//! This module provides utilities to convert proofs generated by circom-prover
//! into the byte format expected by the groth16-solana verifier.
//!
//! # Example
//!
//! ```rust,ignore
//! use circom_prover::{CircomProver, prover::ProofLib, witness::WitnessFn};
//! use groth16_solana::proof_parser::circom_prover::convert_proof;
//!
//! let proof = CircomProver::prove(
//!     ProofLib::Arkworks,
//!     WitnessFn::RustWitness(witness_fn),
//!     circuit_inputs,
//!     zkey_path,
//! )?;
//!
//! let (proof_a, proof_b, proof_c) = convert_proof(&proof.proof)?;
//! ```

#[cfg(feature = "circom")]
pub mod circom_prover {
    use crate::errors::Groth16Error;
    use ark_serialize::{CanonicalSerialize, Compress};
    use solana_bn254::compression::prelude::convert_endianness;
    use std::ops::Neg;

    /// Convert circom-prover proof to groth16-solana format
    ///
    /// This follows the exact pattern from groth16.rs test (lines 347-368):
    /// 1. Serialize with arkworks (outputs LE)
    /// 2. Convert LE to BE using change_endianness/convert_endianness
    ///
    /// # Arguments
    ///
    /// * `circom_proof` - The proof from circom-prover
    ///
    /// # Returns
    ///
    /// A triple of (proof_a, proof_b, proof_c) in the format expected by groth16-solana
    ///
    /// # Errors
    ///
    /// Returns an error if serialization fails or byte conversion fails
    pub fn convert_proof(
        circom_proof: &::circom_prover::prover::circom::Proof,
    ) -> Result<([u8; 64], [u8; 128], [u8; 64]), Groth16Error> {
        // Convert to arkworks proof
        let ark_proof: ark_groth16::Proof<ark_bn254::Bn254> = circom_proof.clone().into();

        // Serialize proof_a: negate it, serialize to LE, convert to BE
        let mut proof_a_serialized = [0u8; 65];
        ark_proof
            .a
            .neg()
            .x
            .serialize_with_mode(&mut proof_a_serialized[..32], Compress::No)?;
        ark_proof
            .a
            .neg()
            .y
            .serialize_with_mode(&mut proof_a_serialized[32..64], Compress::No)?;

        // Convert LE to BE using convert_endianness::<32, 64> (reverses each 32-byte chunk)
        let proof_a: [u8; 64] = convert_endianness::<32, 64>(
            &proof_a_serialized[..64]
                .try_into()
                .map_err(|_| Groth16Error::ProofConversionError)?,
        );

        // Serialize proof_b: serialize to LE, convert to BE
        let mut proof_b_serialized = [0u8; 129];
        ark_proof
            .b
            .serialize_with_mode(&mut proof_b_serialized[..], Compress::No)?;

        // Convert LE to BE using convert_endianness::<64, 128> (reverses each 64-byte chunk)
        let proof_b: [u8; 128] = convert_endianness::<64, 128>(
            &proof_b_serialized[..128]
                .try_into()
                .map_err(|_| Groth16Error::ProofConversionError)?,
        );

        // Serialize proof_c: serialize to LE, convert to BE
        let mut proof_c_serialized = [0u8; 65];
        ark_proof
            .c
            .serialize_with_mode(&mut proof_c_serialized[..], Compress::No)?;

        // Convert LE to BE using convert_endianness::<32, 64> (reverses each 32-byte chunk)
        let proof_c: [u8; 64] = convert_endianness::<32, 64>(
            &proof_c_serialized[..64]
                .try_into()
                .map_err(|_| Groth16Error::ProofConversionError)?,
        );

        Ok((proof_a, proof_b, proof_c))
    }

    /// Convert uncompressed proof to compressed format
    ///
    /// Compresses proof_a (64 bytes -> 32 bytes), proof_b (128 bytes -> 64 bytes),
    /// and proof_c (64 bytes -> 32 bytes) using arkworks compressed serialization.
    ///
    /// # Arguments
    ///
    /// * `proof_a` - Uncompressed proof_a (64 bytes, big-endian)
    /// * `proof_b` - Proof_b (128 bytes, big-endian)
    /// * `proof_c` - Uncompressed proof_c (64 bytes, big-endian)
    ///
    /// # Returns
    ///
    /// A triple of (compressed_proof_a, compressed_proof_b, compressed_proof_c) where:
    /// - compressed_proof_a: 32 bytes
    /// - compressed_proof_b: 64 bytes
    /// - compressed_proof_c: 32 bytes
    ///
    /// # Errors
    ///
    /// Returns an error if deserialization or compression fails
    pub fn convert_proof_to_compressed(
        proof_a: &[u8; 64],
        proof_b: &[u8; 128],
        proof_c: &[u8; 64],
    ) -> Result<([u8; 32], [u8; 64], [u8; 32]), Groth16Error> {
        use solana_bn254::compression::prelude::{alt_bn128_g1_compress, alt_bn128_g2_compress};

        // Compress G1 points using solana_bn254
        let compressed_a =
            alt_bn128_g1_compress(proof_a).map_err(|_| Groth16Error::ProofConversionError)?;

        let compressed_c =
            alt_bn128_g1_compress(proof_c).map_err(|_| Groth16Error::ProofConversionError)?;

        // Compress G2 point using solana_bn254
        let compressed_b =
            alt_bn128_g2_compress(proof_b).map_err(|_| Groth16Error::ProofConversionError)?;

        Ok((compressed_a, compressed_b, compressed_c))
    }

    /// Convert circom-prover public inputs to groth16-solana format
    ///
    /// Circom-prover gives us BigUint in BE format.
    /// Groth16-solana expects BE byte arrays (no conversion needed).
    ///
    /// # Arguments
    ///
    /// * `pub_inputs` - The public inputs from circom-prover
    ///
    /// # Returns
    ///
    /// An array of public inputs in the format expected by groth16-solana
    ///
    /// # Panics
    ///
    /// Panics if the number of public inputs doesn't match N
    pub fn convert_public_inputs<const N: usize>(
        pub_inputs: &::circom_prover::prover::PublicInputs,
    ) -> [[u8; 32]; N] {
        let mut public_inputs_vec: Vec<[u8; 32]> = Vec::new();
        for signal_bigint in &pub_inputs.0 {
            let mut bytes = signal_bigint.to_bytes_be();
            // Pad to 32 bytes
            if bytes.len() < 32 {
                let mut padded = vec![0u8; 32 - bytes.len()];
                padded.extend_from_slice(&bytes);
                bytes = padded;
            }
            public_inputs_vec.push(bytes[..32].try_into().unwrap());
        }
        public_inputs_vec.try_into().unwrap()
    }
}



================================================
FILE: src/vk_parser.rs
================================================
//! Verification key parser for build.rs usage.
//!
//! This module provides utilities to parse verification key JSON files
//! (generated by zkSNARK tooling) and convert them to Rust source code
//! for use with the Groth16 verifier.
//!
//! # Example
//!
//! ```rust,ignore
//! use groth16_solana::vk_parser::generate_vk_file;
//!
//! fn main() {
//!     generate_vk_file(
//!         "verification_key.json",
//!         "src",
//!         "verifying_key.rs"
//!     ).unwrap();
//! }
//! ```

use num_bigint::BigUint;
use serde::Deserialize;
use std::fs;
use std::path::Path;

/// Errors that can occur during verification key parsing
#[derive(Debug, thiserror::Error)]
pub enum VkParseError {
    #[error("Failed to read file: {0}")]
    IoError(#[from] std::io::Error),

    #[error("Failed to parse JSON: {0}")]
    JsonError(#[from] serde_json::Error),

    #[error("Invalid verification key data: {0}")]
    InvalidData(String),
}

/// Raw verification key data as it appears in JSON files
#[derive(Debug, Deserialize)]
struct RawVerifyingKey {
    #[serde(rename = "vk_alpha_1")]
    vk_alpha_1: Vec<String>,

    #[serde(rename = "vk_beta_2")]
    vk_beta_2: Vec<Vec<String>>,

    #[serde(rename = "vk_gamma_2")]
    vk_gamma_2: Vec<Vec<String>>,

    #[serde(rename = "vk_delta_2")]
    vk_delta_2: Vec<Vec<String>>,

    #[serde(rename = "IC")]
    ic: Vec<Vec<String>>,
}

/// Convert a bigint string to little-endian bytes, then reverse to big-endian
fn bigint_string_to_be_bytes(s: &str, size: usize) -> Result<Vec<u8>, VkParseError> {
    let bigint = s
        .parse::<BigUint>()
        .map_err(|e| VkParseError::InvalidData(format!("Failed to parse bigint '{}': {}", s, e)))?;

    let le_bytes = bigint.to_bytes_le();

    // Pad to desired size
    let mut padded = le_bytes;
    padded.resize(size, 0);

    // Reverse to get big-endian
    padded.reverse();
    Ok(padded)
}

/// Process G1 point: convert bigint to 32-byte BE
fn process_g1_component(component: &str) -> Result<Vec<u8>, VkParseError> {
    bigint_string_to_be_bytes(component, 32)
}

/// Process G2 point: concatenate two 32-byte LE components, reverse the full 64 bytes, then split
fn process_g2_component(components: &[String]) -> Result<(Vec<u8>, Vec<u8>), VkParseError> {
    if components.len() != 2 {
        return Err(VkParseError::InvalidData(format!(
            "G2 component must have exactly 2 elements, got {}",
            components.len()
        )));
    }

    // Convert to LE bytes
    let c0_le = components[0]
        .parse::<BigUint>()
        .map_err(|e| VkParseError::InvalidData(format!("Failed to parse bigint: {}", e)))?
        .to_bytes_le();
    let c1_le = components[1]
        .parse::<BigUint>()
        .map_err(|e| VkParseError::InvalidData(format!("Failed to parse bigint: {}", e)))?
        .to_bytes_le();

    // Pad to 32 bytes
    let mut c0_padded = c0_le;
    c0_padded.resize(32, 0);
    let mut c1_padded = c1_le;
    c1_padded.resize(32, 0);

    // Concatenate and reverse
    let mut concat = Vec::new();
    concat.extend_from_slice(&c0_padded);
    concat.extend_from_slice(&c1_padded);
    concat.reverse();

    // Split back
    let part0 = concat[0..32].to_vec();
    let part1 = concat[32..64].to_vec();

    Ok((part0, part1))
}

/// Parse verification key JSON and generate Rust source code as a String
///
/// # Arguments
///
/// * `json_content` - The JSON content as a string
///
/// # Returns
///
/// A String containing the generated Rust code defining a `Groth16Verifyingkey` constant
pub fn parse_vk_json_to_rust_string(json_content: &str) -> Result<String, VkParseError> {
    let raw_vk: RawVerifyingKey = serde_json::from_str(json_content)?;

    let mut output = String::new();

    // Header
    output.push_str("use groth16_solana::groth16::Groth16Verifyingkey;\n\n");
    output.push_str(&format!(
        "pub const VERIFYINGKEY: Groth16Verifyingkey = Groth16Verifyingkey {{\n\tnr_pubinputs: {},\n\n",
        raw_vk.ic.len() - 1
    ));

    // Process vk_alpha_g1 - flat [u8; 64]
    output.push_str("\tvk_alpha_g1: [");
    let mut alpha_bytes = Vec::new();
    for i in 0..raw_vk.vk_alpha_1.len() - 1 {
        let bytes = process_g1_component(&raw_vk.vk_alpha_1[i])?;
        alpha_bytes.extend_from_slice(&bytes);
    }
    output.push_str(
        &alpha_bytes
            .iter()
            .map(|b| format!("{}u8", b))
            .collect::<Vec<_>>()
            .join(", "),
    );
    output.push_str("],\n\n");

    // Process vk_beta_g2 - flat [u8; 128]
    output.push_str("\tvk_beta_g2: [");
    let mut beta_bytes = Vec::new();
    for i in 0..raw_vk.vk_beta_2.len() - 1 {
        let (part0, part1) = process_g2_component(&raw_vk.vk_beta_2[i])?;
        beta_bytes.extend_from_slice(&part0);
        beta_bytes.extend_from_slice(&part1);
    }
    output.push_str(
        &beta_bytes
            .iter()
            .map(|b| format!("{}u8", b))
            .collect::<Vec<_>>()
            .join(", "),
    );
    output.push_str("],\n\n");

    // Process vk_gamma_g2 - flat [u8; 128]
    output.push_str("\tvk_gamma_g2: [");
    let mut gamma_bytes = Vec::new();
    for i in 0..raw_vk.vk_gamma_2.len() - 1 {
        let (part0, part1) = process_g2_component(&raw_vk.vk_gamma_2[i])?;
        gamma_bytes.extend_from_slice(&part0);
        gamma_bytes.extend_from_slice(&part1);
    }
    output.push_str(
        &gamma_bytes
            .iter()
            .map(|b| format!("{}u8", b))
            .collect::<Vec<_>>()
            .join(", "),
    );
    output.push_str("],\n\n");

    // Process vk_delta_g2 - flat [u8; 128]
    output.push_str("\tvk_delta_g2: [");
    let mut delta_bytes = Vec::new();
    for i in 0..raw_vk.vk_delta_2.len() - 1 {
        let (part0, part1) = process_g2_component(&raw_vk.vk_delta_2[i])?;
        delta_bytes.extend_from_slice(&part0);
        delta_bytes.extend_from_slice(&part1);
    }
    output.push_str(
        &delta_bytes
            .iter()
            .map(|b| format!("{}u8", b))
            .collect::<Vec<_>>()
            .join(", "),
    );
    output.push_str("],\n\n");

    // Process vk_ic - &[[u8; 64]]
    output.push_str("\tvk_ic: &[\n");
    for point in &raw_vk.ic {
        output.push_str("\t\t[");
        let mut point_bytes = Vec::new();
        for i in 0..point.len() - 1 {
            let bytes = process_g1_component(&point[i])?;
            point_bytes.extend_from_slice(&bytes);
        }
        output.push_str(
            &point_bytes
                .iter()
                .map(|b| format!("{}u8", b))
                .collect::<Vec<_>>()
                .join(", "),
        );
        output.push_str("],\n");
    }
    output.push_str("\t]\n");

    output.push_str("};\n");

    Ok(output)
}

/// Generate a verification key Rust file from a JSON file
///
/// This is a convenience wrapper that reads the JSON file, parses it,
/// and writes the generated Rust code to the specified output location.
///
/// # Arguments
///
/// * `json_path` - Path to the input JSON file containing the verification key
/// * `output_dir` - Directory where the output Rust file will be written
/// * `output_filename` - Name of the output Rust file (e.g., "verifying_key.rs")
///
/// # Example
///
/// ```rust,ignore
/// // In build.rs
/// use groth16_solana::vk_parser::generate_vk_file;
///
/// fn main() {
///     generate_vk_file(
///         "verification_key.json",
///         "src",
///         "verifying_key.rs"
///     ).unwrap();
/// }
/// ```
pub fn generate_vk_file(
    json_path: impl AsRef<Path>,
    output_dir: impl AsRef<Path>,
    output_filename: &str,
) -> Result<(), VkParseError> {
    // Read JSON file
    let json_content = fs::read_to_string(json_path.as_ref())?;

    // Parse and generate Rust code
    let rust_code = parse_vk_json_to_rust_string(&json_content)?;

    // Create output directory if it doesn't exist
    fs::create_dir_all(output_dir.as_ref())?;

    // Write output file
    let output_path = output_dir.as_ref().join(output_filename);
    fs::write(output_path, rust_code)?;

    Ok(())
}



================================================
FILE: tests/rust-vk/README.md
================================================
# groth16-solana Integration Test with Rust VK Parser

This integration test demonstrates the complete workflow for using `groth16-solana` with the Rust verification key parser:

1. Compile a circom circuit
2. Generate proving and verification keys
3. Use the Rust VK parser (via `build.rs`) to generate the verifying key Rust code
4. Generate a proof
5. Verify the proof using `groth16-solana`

## Prerequisites

- Rust (latest stable)
- Node.js and npm
- circom (install with `npm install -g circom`)

## Setup

1. Install dependencies and download powers of tau:

```bash
npm run setup
```

2. Compile the circuit and generate keys:

```bash
npm run build-all
```

This will:
- Compile the circom circuit to R1CS and WASM
- Run the Groth16 trusted setup
- Contribute entropy to the ceremony
- Export the verification key JSON

## Running the Test

The test will automatically use the generated verification key via the `build.rs` script:

```bash
cargo test
```

## How It Works

### Build Process

The `build.rs` script does two things:

1. **Generates Verification Key Rust Code**:
   - Reads `build/verification_key.json`
   - Uses `groth16_solana::vk_parser::generate_vk_file()` to convert it to Rust
   - Outputs `src/verifying_key.rs` containing the `VERIFYINGKEY` constant

2. **Transpiles Witness Generator**:
   - Converts the WASM witness generator to a native Rust library
   - Links it for use with `circom-prover`

### Test Flow

The integration test (`src/lib.rs`):

1. Creates a compressed account and Merkle proof inputs
2. Generates a Groth16 proof using `circom-prover`
3. Converts the proof to the format expected by `groth16-solana`
4. Verifies the proof using `Groth16Verifier` with the generated `VERIFYINGKEY`

## Project Structure

```
tests/rust-vk/
├── build/                    # Generated circuit files
│   ├── compressed_account_merkle_proof.r1cs
│   ├── compressed_account_merkle_proof_js/  # WASM witness generator
│   ├── compressed_account_merkle_proof_final.zkey
│   └── verification_key.json
├── circuits/
│   └── compressed_account_merkle_proof.circom
├── pot/
│   └── powersOfTau28_hez_final_16.ptau
├── scripts/
│   └── setup.sh
├── src/
│   ├── lib.rs              # Integration test
│   └── verifying_key.rs    # Generated by build.rs
├── build.rs                # Uses groth16-solana VK parser
├── Cargo.toml
└── package.json
```

## Key Features Demonstrated

1. **Rust VK Parser Integration**: Shows how to use the `vk` feature of `groth16-solana` in `build.rs`
2. **Complete Circuit Workflow**: From circom source to verified proof
3. **Proof Format Conversion**: Demonstrates converting between circom-prover and groth16-solana formats
4. **Build-time Code Generation**: Verification key is generated at compile time

## Troubleshooting

### "Verification key JSON not found"

Run `npm run build-all` to generate the circuit artifacts.

### "Witness WASM not found"

Run `npm run compile` to compile the circuit.

### Verification fails

Ensure you've run the full setup and the verification key matches the proving key:
```bash
npm run clean
npm run build-all
cargo clean
cargo test
```

## Notes

- The circuit uses a 26-level Merkle tree for compressed account proofs
- The powers of tau ceremony file supports circuits up to 2^16 constraints
- The verification key is regenerated whenever `build/verification_key.json` changes



================================================
FILE: tests/rust-vk/build.rs
================================================
use groth16_solana::vk_parser::generate_vk_file;

fn main() {
    println!("cargo:rerun-if-changed=build/verification_key.json");
    println!("cargo:rerun-if-changed=build/compressed_account_merkle_proof_js");

    // Generate the verifying key Rust file from the JSON
    // This will be generated after running the circuit setup scripts
    let vk_json_path = "./build/verification_key.json";
    let output_dir = "./src";
    let output_file = "verifying_key.rs";

    // Only generate if the verification_key.json exists
    if std::path::Path::new(vk_json_path).exists() {
        generate_vk_file(vk_json_path, output_dir, output_file)
            .expect("Failed to generate verifying key Rust file");
        println!("cargo:warning=Generated verifying_key.rs from verification_key.json");
    } else {
        println!("cargo:warning=Verification key JSON not found. Run 'npm run build-all' first.");
    }

    // Transpile the WebAssembly witness generator to native library
    let witness_wasm_dir = "./build/compressed_account_merkle_proof_js";
    if std::path::Path::new(witness_wasm_dir).exists() {
        rust_witness::transpile::transpile_wasm(witness_wasm_dir.to_string());
        println!("cargo:warning=Transpiled witness generator");
    } else {
        println!("cargo:warning=Witness WASM not found. Run 'npm run compile' first.");
    }
}



================================================
FILE: tests/rust-vk/Cargo.toml
================================================
[package]
name = "rust-vk-integration-test"
version = "0.1.0"
edition = "2021"
publish = false

[dependencies]
groth16-solana = { path = "../..", features = ["circom"] }
circom-prover = "0.1"
rust-witness = "0.1"
num-bigint = "0.4"
serde_json = "1.0"
light-compressed-account = { version = "0.5.0", features = ["new-unique"] }
light-hasher = { version = "4.0.0", features = ["solana"] }
light-merkle-tree-reference = "3.0.0"

[build-dependencies]
groth16-solana = { path = "../..", features = ["vk", "circom"] }
rust-witness = "0.1"



================================================
FILE: tests/rust-vk/package.json
================================================
{
  "name": "groth16-solana-rust-vk-test",
  "version": "1.0.0",
  "description": "Integration test for groth16-solana with Rust VK parser",
  "scripts": {
    "setup": "./scripts/setup.sh",
    "clean": "rm -rf build/* node_modules",
    "compile": "mkdir -p build && circom circuits/compressed_account_merkle_proof.circom --r1cs --wasm --sym -o build",
    "setup-circuit": "snarkjs groth16 setup build/compressed_account_merkle_proof.r1cs pot/powersOfTau28_hez_final_16.ptau build/compressed_account_merkle_proof_0000.zkey",
    "contribute": "snarkjs zkey contribute build/compressed_account_merkle_proof_0000.zkey build/compressed_account_merkle_proof_final.zkey --name='Test contribution' -e='random entropy'",
    "export-vkey": "snarkjs zkey export verificationkey build/compressed_account_merkle_proof_final.zkey build/verification_key.json",
    "build-all": "npm run setup && npm run compile && npm run setup-circuit && npm run contribute && npm run export-vkey"
  },
  "dependencies": {
    "circomlib": "2.0.5"
  },
  "devDependencies": {
    "snarkjs": "^0.7.0"
  }
}



================================================
FILE: tests/rust-vk/circuits/README.md
================================================
# ZK Circom Circuit for Compressed Account Merkle Proof

This directory contains a circom circuit that verifies a Merkle proof of a compressed account on Solana.

## Circuit Overview

The `compressed_account_merkle_proof.circom` circuit combines two key components:

1. **Compressed Account Hash**: Computes the hash of a compressed account using Poseidon hash
   - Based on: https://github.com/ananas-block/compressed-account-circuit
   - Inputs: owner_hashed, leaf_index, merkle_tree_hashed, discriminator, data_hash

2. **Merkle Proof Verification**: Verifies the account exists in a Merkle tree
   - Based on: /Users/ananas/dev/light-protocol2/circuit-lib/circuit-lib.circom
   - Inputs: pathElements (26 levels), expectedRoot

## Setup

Run the setup script - it will handle dependencies, compilation, and key generation:

```bash
./scripts/setup.sh
```

To clean up build artifacts:

```bash
./scripts/clean.sh
```

## Testing

### Rust Test with Mopro

The circuit can be tested from Rust using the mopro library:

```bash
cargo test test_compressed_account_merkle_proof_circuit
```

This test:
1. Loads the compiled circuit and zkey
2. Generates a proof with sample inputs
3. Verifies the proof is valid

## Circuit Structure

```
CompressedAccountMerkleProof (main)
├── CompressedAccountHash
│   └── Poseidon(5) - Hashes account fields
└── MerkleProof(26)
    ├── Num2Bits - Converts leaf index to bits
    ├── Switcher[26] - Routes left/right based on path
    └── Poseidon(2)[26] - Hashes up the tree
```

## Public Inputs

The following inputs are public (visible in the proof):
- owner_hashed
- merkle_tree_hashed
- discriminator
- data_hash
- expectedRoot

Private inputs:
- leaf_index
- pathElements

## References

- Compressed Account Circuit: https://github.com/ananas-block/compressed-account-circuit
- Merkle Proof Implementation: /Users/ananas/dev/light-protocol2/circuit-lib/circuit-lib.circom
- Mopro ZK Library: https://github.com/zkmopro/mopro
- SnarkJS: https://github.com/iden3/snarkjs



================================================
FILE: tests/rust-vk/circuits/compressed_account_merkle_proof.circom
================================================
pragma circom 2.0.0;

include "../node_modules/circomlib/circuits/poseidon.circom";
include "../node_modules/circomlib/circuits/bitify.circom";
include "../node_modules/circomlib/circuits/switcher.circom";
include "../node_modules/circomlib/circuits/comparators.circom";

// Compressed Account Hash Template
// Computes the hash of a compressed account
template CompressedAccountHash() {
    signal input owner_hashed;
    signal input leaf_index;
    signal input merkle_tree_hashed;
    signal input discriminator;
    signal input data_hash;

    signal output hash;

    component poseidon = Poseidon(5);

    poseidon.inputs[0] <== owner_hashed;
    poseidon.inputs[1] <== leaf_index;
    poseidon.inputs[2] <== merkle_tree_hashed;
    poseidon.inputs[3] <== discriminator + 36893488147419103232; // + discriminator domain
    poseidon.inputs[4] <== data_hash;

    hash <== poseidon.out;
}

// Merkle Proof Verification Template
// Verifies that a leaf is in a Merkle tree with a given root
template MerkleProof(levels) {
    signal input leaf;
    signal input pathElements[levels];
    signal input leafIndex;
    signal output root;

    component switcher[levels];
    component hasher[levels];

    component indexBits = Num2Bits(levels);
    indexBits.in <== leafIndex;

    for (var i = 0; i < levels; i++) {
        switcher[i] = Switcher();
        switcher[i].L <== i == 0 ? leaf : hasher[i - 1].out;
        switcher[i].R <== pathElements[i];
        switcher[i].sel <== indexBits.out[i];

        hasher[i] = Poseidon(2);
        hasher[i].inputs[0] <== switcher[i].outL;
        hasher[i].inputs[1] <== switcher[i].outR;
    }

    root <== hasher[levels - 1].out;
}

// Main Circuit: Compressed Account Merkle Proof Verification
// Computes compressed account hash and verifies it exists in a Merkle tree
template CompressedAccountMerkleProof(levels) {
    // Compressed account inputs
    signal input owner_hashed;
    signal input leaf_index;
    signal input merkle_tree_hashed;
    signal input discriminator;
    signal input data_hash;

    // Merkle proof inputs
    signal input pathElements[levels];
    signal input expectedRoot;

    // Step 1: Compute compressed account hash
    component accountHasher = CompressedAccountHash();
    accountHasher.owner_hashed <== owner_hashed;
    accountHasher.leaf_index <== leaf_index;
    accountHasher.merkle_tree_hashed <== merkle_tree_hashed;
    accountHasher.discriminator <== discriminator;
    accountHasher.data_hash <== data_hash;

    // Step 2: Verify Merkle proof
    component merkleProof = MerkleProof(levels);
    merkleProof.leaf <== accountHasher.hash;
    merkleProof.pathElements <== pathElements;
    merkleProof.leafIndex <== leaf_index;

    // Step 3: CRITICAL CONSTRAINT - Enforce that computed root MUST equal expected root
    // This === operator adds a constraint that will fail witness generation if roots don't match
    merkleProof.root === expectedRoot;
}

// Main component with 26 levels (typical for Solana state trees)
component main {
    public [
        owner_hashed,
        merkle_tree_hashed,
        discriminator,
        data_hash,
        expectedRoot
    ]
} = CompressedAccountMerkleProof(26);



================================================
FILE: tests/rust-vk/scripts/setup.sh
================================================
#!/bin/bash

set -e

echo "Setting up groth16-solana integration test..."

# Create directories
mkdir -p pot build

# Download powers of tau if not exists
POT_FILE="pot/powersOfTau28_hez_final_16.ptau"
if [ ! -f "$POT_FILE" ]; then
    echo "Downloading powers of tau ceremony file..."
    curl -L https://storage.googleapis.com/zkevm/ptau/powersOfTau28_hez_final_16.ptau -o "$POT_FILE"
    echo "Powers of tau downloaded successfully"
else
    echo "Powers of tau file already exists, skipping download"
fi

# Install npm dependencies
echo "Installing npm dependencies..."
npm install

echo "Setup complete! Run 'npm run build-all' to compile the circuit and generate keys"



================================================
FILE: tests/rust-vk/src/lib.rs
================================================
#![allow(unused)]
use circom_prover::{prover::ProofLib, witness::WitnessFn, CircomProver};
use groth16_solana::groth16::Groth16Verifier;
use groth16_solana::proof_parser::circom_prover::{convert_proof, convert_public_inputs};
use light_compressed_account::compressed_account::{CompressedAccount, CompressedAccountData};
use light_compressed_account::Pubkey;
use light_hasher::{hash_to_field_size::hash_to_bn254_field_size_be, Poseidon};
use light_merkle_tree_reference::MerkleTree;
use num_bigint::BigUint;
use std::collections::HashMap;

// Link the generated witness library
#[link(name = "circuit", kind = "static")]
extern "C" {}

// Declare the witness function
rust_witness::witness!(compressedaccountmerkleproof);

// Include the generated verifying key
// This will be generated by build.rs from the verification_key.json
mod verifying_key;

use verifying_key::VERIFYINGKEY;

/// Helper function to add compressed account inputs to the circuit inputs HashMap
fn add_compressed_account_to_circuit_inputs(
    inputs: &mut HashMap<String, Vec<String>>,
    compressed_account: &CompressedAccount,
    merkle_tree_pubkey: &Pubkey,
    leaf_index: u32,
) {
    let owner = compressed_account.owner;
    let (discriminator, data_hash) = if let Some(ref data) = compressed_account.data {
        (data.discriminator, data.data_hash)
    } else {
        ([0u8; 8], [0u8; 32])
    };

    let owner_hashed = hash_to_bn254_field_size_be(owner.as_ref());
    let merkle_tree_hashed = hash_to_bn254_field_size_be(merkle_tree_pubkey.as_ref());

    inputs.insert(
        "owner_hashed".to_string(),
        vec![BigUint::from_bytes_be(&owner_hashed).to_string()],
    );
    inputs.insert("leaf_index".to_string(), vec![leaf_index.to_string()]);
    inputs.insert(
        "merkle_tree_hashed".to_string(),
        vec![BigUint::from_bytes_be(&merkle_tree_hashed).to_string()],
    );
    inputs.insert(
        "discriminator".to_string(),
        vec![BigUint::from_bytes_be(&discriminator).to_string()],
    );
    inputs.insert(
        "data_hash".to_string(),
        vec![BigUint::from_bytes_be(&data_hash).to_string()],
    );

    println!("Public inputs (in circuit order):");
    println!("  [0] owner_hashed: {:?}", owner_hashed);
    println!("  [1] merkle_tree_hashed: {:?}", merkle_tree_hashed);
    println!("  [2] discriminator: {:?}", discriminator);
    println!("  [3] data_hash: {:?}", data_hash);
}

/// Helper function to add Merkle proof inputs to the circuit inputs HashMap
fn add_merkle_proof_to_circuit_inputs(
    inputs: &mut HashMap<String, Vec<String>>,
    merkle_proof_hashes: &[[u8; 32]],
    merkle_root: &[u8; 32],
) {
    let path_elements: Vec<String> = merkle_proof_hashes
        .iter()
        .map(|hash| BigUint::from_bytes_be(hash).to_string())
        .collect();
    inputs.insert("pathElements".to_string(), path_elements);

    let expected_root_bigint = BigUint::from_bytes_be(merkle_root);
    inputs.insert(
        "expectedRoot".to_string(),
        vec![expected_root_bigint.to_string()],
    );

    println!("  [4] expectedRoot: {:?}", merkle_root);
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_compressed_account_proof_with_groth16_solana() {
        let zkey_path = "./build/compressed_account_merkle_proof_final.zkey".to_string();

        // Create compressed account
        let owner = Pubkey::new_from_array([1u8; 32]);
        let merkle_tree_pubkey = Pubkey::new_from_array([2u8; 32]);
        let leaf_index: u32 = 0;

        let compressed_account = CompressedAccount {
            owner,
            lamports: 0,
            address: None,
            data: Some(CompressedAccountData {
                discriminator: [1u8; 8],
                data: vec![],
                data_hash: [3u8; 32],
            }),
        };

        // Create Merkle tree and get proof
        let compressed_account_hash = compressed_account
            .hash(&merkle_tree_pubkey, &leaf_index, false)
            .unwrap();

        let mut merkle_tree = MerkleTree::<Poseidon>::new(26, 0);
        merkle_tree.append(&compressed_account_hash).unwrap();

        let merkle_proof_hashes = merkle_tree
            .get_proof_of_leaf(leaf_index as usize, false)
            .unwrap();
        let merkle_root = merkle_tree.root();

        // Build circuit inputs
        let mut proof_inputs = HashMap::new();
        add_compressed_account_to_circuit_inputs(
            &mut proof_inputs,
            &compressed_account,
            &merkle_tree_pubkey,
            leaf_index,
        );
        add_merkle_proof_to_circuit_inputs(&mut proof_inputs, &merkle_proof_hashes, &merkle_root);

        // Generate proof using circom-prover
        let circuit_inputs = serde_json::to_string(&proof_inputs).unwrap();
        let proof = CircomProver::prove(
            ProofLib::Arkworks,
            WitnessFn::RustWitness(compressedaccountmerkleproof_witness),
            circuit_inputs,
            zkey_path.clone(),
        )
        .expect("Proof generation failed");
        println!(
            "Proof generated successfully with circom-prover {:?}",
            proof
        );
        // First verify the proof with circom-prover to ensure it's valid
        let is_valid_circom =
            CircomProver::verify(ProofLib::Arkworks, proof.clone(), zkey_path.clone())
                .expect("Circom proof verification failed");
        assert!(
            is_valid_circom,
            "Proof should be valid according to circom-prover"
        );
        println!("Proof verified successfully with circom-prover");

        // Convert proof and public inputs to groth16-solana format
        let (proof_a, proof_b, proof_c) =
            convert_proof(&proof.proof).expect("Failed to convert proof to groth16-solana format");
        let public_inputs: [[u8; 32]; 5] = convert_public_inputs(&proof.pub_inputs);
        println!("Public inputs for groth16-solana: {:?}", public_inputs);
        println!("Proof A: {:?}", proof_a);
        println!("Proof B: {:?}", proof_b);
        println!("Proof C: {:?}", proof_c);
        let mut verifier =
            Groth16Verifier::new(&proof_a, &proof_b, &proof_c, &public_inputs, &VERIFYINGKEY)
                .expect("Failed to create verifier");

        verifier.verify().expect("Proof verification failed");

        println!("Proof verified successfully with groth16-solana!");
    }
}



================================================
FILE: tests/rust-vk/src/verifying_key.rs
================================================
use groth16_solana::groth16::Groth16Verifyingkey;

pub const VERIFYINGKEY: Groth16Verifyingkey = Groth16Verifyingkey {
	nr_pubinputs: 5,

	vk_alpha_g1: [45u8, 77u8, 154u8, 167u8, 227u8, 2u8, 217u8, 223u8, 65u8, 116u8, 157u8, 85u8, 7u8, 148u8, 157u8, 5u8, 219u8, 234u8, 51u8, 251u8, 177u8, 108u8, 100u8, 59u8, 34u8, 245u8, 153u8, 162u8, 190u8, 109u8, 242u8, 226u8, 20u8, 190u8, 221u8, 80u8, 60u8, 55u8, 206u8, 176u8, 97u8, 216u8, 236u8, 96u8, 32u8, 159u8, 227u8, 69u8, 206u8, 137u8, 131u8, 10u8, 25u8, 35u8, 3u8, 1u8, 240u8, 118u8, 202u8, 255u8, 0u8, 77u8, 25u8, 38u8],

	vk_beta_g2: [9u8, 103u8, 3u8, 47u8, 203u8, 247u8, 118u8, 209u8, 175u8, 201u8, 133u8, 248u8, 136u8, 119u8, 241u8, 130u8, 211u8, 132u8, 128u8, 166u8, 83u8, 242u8, 222u8, 202u8, 169u8, 121u8, 76u8, 188u8, 59u8, 243u8, 6u8, 12u8, 14u8, 24u8, 120u8, 71u8, 173u8, 76u8, 121u8, 131u8, 116u8, 208u8, 214u8, 115u8, 43u8, 245u8, 1u8, 132u8, 125u8, 214u8, 139u8, 192u8, 224u8, 113u8, 36u8, 30u8, 2u8, 19u8, 188u8, 127u8, 193u8, 61u8, 183u8, 171u8, 48u8, 76u8, 251u8, 209u8, 224u8, 138u8, 112u8, 74u8, 153u8, 245u8, 232u8, 71u8, 217u8, 63u8, 140u8, 60u8, 170u8, 253u8, 222u8, 196u8, 107u8, 122u8, 13u8, 55u8, 157u8, 166u8, 154u8, 77u8, 17u8, 35u8, 70u8, 167u8, 23u8, 57u8, 193u8, 177u8, 164u8, 87u8, 168u8, 199u8, 49u8, 49u8, 35u8, 210u8, 77u8, 47u8, 145u8, 146u8, 248u8, 150u8, 183u8, 198u8, 62u8, 234u8, 5u8, 169u8, 213u8, 127u8, 6u8, 84u8, 122u8, 208u8, 206u8, 200u8],

	vk_gamma_g2: [25u8, 142u8, 147u8, 147u8, 146u8, 13u8, 72u8, 58u8, 114u8, 96u8, 191u8, 183u8, 49u8, 251u8, 93u8, 37u8, 241u8, 170u8, 73u8, 51u8, 53u8, 169u8, 231u8, 18u8, 151u8, 228u8, 133u8, 183u8, 174u8, 243u8, 18u8, 194u8, 24u8, 0u8, 222u8, 239u8, 18u8, 31u8, 30u8, 118u8, 66u8, 106u8, 0u8, 102u8, 94u8, 92u8, 68u8, 121u8, 103u8, 67u8, 34u8, 212u8, 247u8, 94u8, 218u8, 221u8, 70u8, 222u8, 189u8, 92u8, 217u8, 146u8, 246u8, 237u8, 9u8, 6u8, 137u8, 208u8, 88u8, 95u8, 240u8, 117u8, 236u8, 158u8, 153u8, 173u8, 105u8, 12u8, 51u8, 149u8, 188u8, 75u8, 49u8, 51u8, 112u8, 179u8, 142u8, 243u8, 85u8, 172u8, 218u8, 220u8, 209u8, 34u8, 151u8, 91u8, 18u8, 200u8, 94u8, 165u8, 219u8, 140u8, 109u8, 235u8, 74u8, 171u8, 113u8, 128u8, 141u8, 203u8, 64u8, 143u8, 227u8, 209u8, 231u8, 105u8, 12u8, 67u8, 211u8, 123u8, 76u8, 230u8, 204u8, 1u8, 102u8, 250u8, 125u8, 170u8],

	vk_delta_g2: [29u8, 101u8, 86u8, 247u8, 6u8, 195u8, 12u8, 191u8, 43u8, 222u8, 235u8, 242u8, 147u8, 233u8, 121u8, 93u8, 83u8, 252u8, 44u8, 61u8, 94u8, 229u8, 105u8, 232u8, 9u8, 253u8, 67u8, 31u8, 174u8, 9u8, 16u8, 187u8, 27u8, 125u8, 210u8, 232u8, 114u8, 142u8, 123u8, 67u8, 44u8, 126u8, 11u8, 182u8, 127u8, 1u8, 20u8, 144u8, 1u8, 164u8, 61u8, 63u8, 135u8, 177u8, 180u8, 193u8, 178u8, 98u8, 206u8, 69u8, 227u8, 229u8, 89u8, 66u8, 41u8, 82u8, 242u8, 150u8, 164u8, 228u8, 129u8, 0u8, 233u8, 115u8, 15u8, 189u8, 40u8, 32u8, 52u8, 82u8, 239u8, 56u8, 158u8, 185u8, 139u8, 27u8, 72u8, 139u8, 122u8, 110u8, 89u8, 64u8, 244u8, 137u8, 155u8, 220u8, 27u8, 62u8, 173u8, 25u8, 4u8, 255u8, 70u8, 49u8, 164u8, 34u8, 141u8, 154u8, 59u8, 27u8, 209u8, 25u8, 232u8, 157u8, 58u8, 143u8, 148u8, 105u8, 178u8, 66u8, 226u8, 156u8, 68u8, 197u8, 234u8, 139u8, 5u8, 176u8],

	vk_ic: &[
		[12u8, 192u8, 31u8, 5u8, 252u8, 153u8, 69u8, 152u8, 16u8, 193u8, 229u8, 27u8, 107u8, 12u8, 131u8, 157u8, 181u8, 114u8, 177u8, 46u8, 213u8, 38u8, 159u8, 73u8, 102u8, 40u8, 81u8, 214u8, 58u8, 242u8, 73u8, 236u8, 0u8, 58u8, 182u8, 252u8, 156u8, 30u8, 52u8, 250u8, 147u8, 222u8, 206u8, 111u8, 25u8, 4u8, 107u8, 115u8, 113u8, 133u8, 43u8, 77u8, 19u8, 249u8, 34u8, 131u8, 134u8, 29u8, 211u8, 140u8, 237u8, 147u8, 30u8, 7u8],
		[9u8, 110u8, 51u8, 213u8, 168u8, 169u8, 164u8, 244u8, 19u8, 76u8, 14u8, 226u8, 213u8, 125u8, 216u8, 90u8, 119u8, 166u8, 116u8, 61u8, 176u8, 65u8, 159u8, 48u8, 229u8, 17u8, 174u8, 208u8, 151u8, 239u8, 129u8, 240u8, 31u8, 23u8, 226u8, 142u8, 142u8, 189u8, 121u8, 137u8, 47u8, 204u8, 35u8, 136u8, 216u8, 198u8, 164u8, 12u8, 93u8, 183u8, 45u8, 207u8, 112u8, 240u8, 113u8, 41u8, 111u8, 19u8, 197u8, 177u8, 167u8, 71u8, 172u8, 125u8],
		[42u8, 196u8, 14u8, 64u8, 95u8, 212u8, 162u8, 19u8, 5u8, 80u8, 227u8, 221u8, 1u8, 190u8, 42u8, 223u8, 97u8, 165u8, 33u8, 109u8, 138u8, 210u8, 40u8, 82u8, 231u8, 37u8, 29u8, 98u8, 180u8, 32u8, 101u8, 225u8, 38u8, 201u8, 17u8, 234u8, 168u8, 4u8, 165u8, 151u8, 1u8, 204u8, 196u8, 33u8, 147u8, 161u8, 210u8, 91u8, 200u8, 239u8, 230u8, 152u8, 163u8, 3u8, 232u8, 1u8, 33u8, 55u8, 28u8, 149u8, 241u8, 133u8, 25u8, 207u8],
		[37u8, 193u8, 7u8, 139u8, 47u8, 18u8, 214u8, 151u8, 196u8, 40u8, 110u8, 111u8, 149u8, 1u8, 58u8, 190u8, 105u8, 197u8, 122u8, 12u8, 98u8, 38u8, 24u8, 5u8, 123u8, 31u8, 26u8, 50u8, 18u8, 175u8, 245u8, 140u8, 26u8, 182u8, 233u8, 126u8, 204u8, 44u8, 225u8, 78u8, 38u8, 138u8, 242u8, 127u8, 3u8, 31u8, 89u8, 84u8, 137u8, 139u8, 182u8, 94u8, 88u8, 190u8, 177u8, 253u8, 207u8, 84u8, 232u8, 115u8, 231u8, 205u8, 235u8, 186u8],
		[21u8, 10u8, 233u8, 74u8, 135u8, 47u8, 118u8, 170u8, 135u8, 5u8, 211u8, 202u8, 125u8, 176u8, 78u8, 204u8, 7u8, 204u8, 181u8, 215u8, 166u8, 122u8, 3u8, 40u8, 49u8, 99u8, 115u8, 191u8, 83u8, 134u8, 58u8, 69u8, 3u8, 121u8, 255u8, 73u8, 110u8, 240u8, 240u8, 60u8, 219u8, 180u8, 124u8, 27u8, 165u8, 0u8, 88u8, 80u8, 217u8, 152u8, 34u8, 246u8, 34u8, 195u8, 148u8, 37u8, 207u8, 236u8, 0u8, 120u8, 154u8, 60u8, 152u8, 209u8],
		[42u8, 66u8, 27u8, 187u8, 193u8, 14u8, 48u8, 193u8, 195u8, 71u8, 164u8, 58u8, 40u8, 58u8, 74u8, 151u8, 2u8, 132u8, 65u8, 86u8, 4u8, 102u8, 73u8, 22u8, 113u8, 71u8, 235u8, 55u8, 3u8, 121u8, 70u8, 184u8, 42u8, 243u8, 12u8, 20u8, 194u8, 124u8, 62u8, 60u8, 228u8, 53u8, 208u8, 43u8, 171u8, 182u8, 240u8, 3u8, 45u8, 105u8, 185u8, 34u8, 252u8, 46u8, 251u8, 217u8, 171u8, 226u8, 140u8, 124u8, 186u8, 183u8, 35u8, 194u8],
	]
};



================================================
FILE: .github/actions/setup/action.yml
================================================
name: Setup Environment
description: Setup Rust, Node.js, Circom and SnarkJS for testing

inputs:
  rust-toolchain:
    description: "Rust toolchain version"
    required: false
    default: "1.90.0"
  node-version:
    description: "Node.js version"
    required: false
    default: "22"
  circom-version:
    description: "Circom version"
    required: false
    default: "v2.2.2"

runs:
  using: composite
  steps:
    - name: Install Rust toolchain
      uses: actions-rust-lang/setup-rust-toolchain@v1
      with:
        toolchain: ${{ inputs.rust-toolchain }}

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ inputs.node-version }}

    - name: Cache circom installation
      id: cache-circom
      uses: actions/cache@v4
      with:
        path: ~/.cargo/bin/circom
        key: circom-${{ runner.os }}-${{ inputs.circom-version }}

    - name: Install circom
      if: steps.cache-circom.outputs.cache-hit != 'true'
      shell: bash
      run: |
        wget -q https://github.com/iden3/circom/releases/download/${{ inputs.circom-version }}/circom-linux-amd64
        chmod +x circom-linux-amd64
        mv circom-linux-amd64 ~/.cargo/bin/circom

    - name: Install snarkjs
      shell: bash
      run: npm install -g snarkjs

    - name: Display versions
      shell: bash
      run: |
        rustc --version
        cargo --version
        node --version
        npm --version
        circom --version
        snarkjs --help | head -1 || true



================================================
FILE: .github/workflows/rust-tests.yml
================================================
name: Rust

on:
  push:
    branches:
      - master
  pull_request:
    types: [opened, synchronize, reopened]
    branches:
      - master

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  RUST_TOOLCHAIN: "1.90.0"
  NODE_VERSION: "22"
  CIRCOM_VERSION: "v2.2.2"

jobs:
  test-rust:
    name: Test Workspace
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup environment
        uses: ./.github/actions/setup
        with:
          rust-toolchain: ${{ env.RUST_TOOLCHAIN }}
          node-version: ${{ env.NODE_VERSION }}
          circom-version: ${{ env.CIRCOM_VERSION }}

      - name: Install npm dependencies (for circuit compilation)
        working-directory: tests/rust-vk
        run: npm install

      - name: Compile circuit and generate keys
        working-directory: tests/rust-vk
        run: npm run build-all

      - name: Build workspace
        run: cargo build --workspace

      - name: Test workspace
        run: cargo test --workspace


